# Chapter 2: Real-World Turn Detection - Streaming Behavior & Robustness

> **Beyond Test Accuracy**: What happens when these models meet real ASR systems?

---

## üéØ The Question

Chapter 1 showed that our Domain model achieved **100% accuracy** on call center test data. But accuracy alone doesn't tell us how the model will perform in production.

In real-time conversation systems:
1. Text arrives **word-by-word** from ASR/STT systems
2. Punctuation may arrive **late or not at all** (depending on ASR quality)
3. We need **stable, confident predictions** to avoid interrupting users

This chapter investigates two critical questions:
- **How do predictions evolve** as words stream in?
- **Are we dependent on punctuation** like other text-based turn detectors?

---

## üìä Part 1: Streaming Inference Behavior

### How Models Make Decisions Word-by-Word

Below are real examples from our call center test set, showing how predictions change as each word arrives.

#### Example 1: Short Question

**Sentence:** "Is it a 250 or 350?"  
**True Label:** Complete ‚úì

| Word(s) | General Model | General Confidence | Domain Model | Domain Confidence |
|---------|---------------|-------------------|--------------|-------------------|
| Is | Incomplete | 97.7% | Incomplete | 99.9% |
| Is it | Incomplete | 97.7% | Incomplete | 99.9% |
| Is it a | Incomplete | 97.7% | Incomplete | 99.9% |
| Is it a 250 | Incomplete | 97.6% | Incomplete | 99.9% |
| Is it a 250 or | Incomplete | 97.7% | Incomplete | 99.9% |
| Is it a 250 or 350? | **Complete** | 97.9% | **Complete** | 100.0% |

**Observation:** Both models hold "Incomplete" steadily through "or", then flip to "Complete" when the **question mark** arrives.

---

#### Example 2: Trailing Phrase

**Sentence:** "We were hoping kind of"  
**True Label:** Incomplete ‚úì

| Word(s) | General Model | General Confidence | Domain Model | Domain Confidence |
|---------|---------------|-------------------|--------------|-------------------|
| We | Incomplete | 97.7% | Incomplete | 99.9% |
| We were | Incomplete | 97.7% | Incomplete | 99.9% |
| We were hoping | Incomplete | 97.7% | Incomplete | 99.9% |
| We were hoping kind | Incomplete | 97.7% | Incomplete | 99.9% |
| We were hoping kind of | **Incomplete** | 96.8% | **Incomplete** | 99.9% |

**Observation:** "kind of" signals incompleteness - both models stay confident. General model dips to 96.8%, Domain stays at 99.9%.

---

#### Example 3: Long Utterance with Fluctuation

**Sentence:** "And get with my and see now if we can put a deal together."  
**True Label:** Complete ‚úì

| Word(s) | General Model | General Confidence | Domain Model | Domain Confidence |
|---------|---------------|-------------------|--------------|-------------------|
| And | Incomplete | 97.7% | Incomplete | 99.9% |
| And get | Incomplete | 97.7% | Incomplete | 99.9% |
| And get with | Incomplete | 97.7% | Incomplete | 99.9% |
| And get with my | Incomplete | 97.7% | Incomplete | 99.9% |
| And get with my and | Incomplete | 97.7% | Incomplete | 99.9% |
| And get with my and see | Incomplete | 97.7% | Incomplete | 99.9% |
| And get with my and see now | Incomplete | 96.7% | Incomplete | 99.9% |
| And get with my and see now if | Incomplete | 94.0% | Incomplete | 99.9% |
| And get with my and see now if we | Incomplete | 95.6% | Incomplete | 99.9% |
| And get with my and see now if we can | Incomplete | **69.3%** | Incomplete | 99.9% |
| And get with my and see now if we can put | Incomplete | 93.1% | Incomplete | 99.9% |
| And get with my and see now if we can put a | Incomplete | 95.7% | Incomplete | 99.9% |
| And get with my and see now if we can put a deal | Incomplete | **54.0%** | Incomplete | 99.9% |
| And get with my and see now if we can put a deal together. | **Complete** | 97.9% | **Complete** | 100.0% |

**Critical Observation:** 
- General model's confidence **fluctuates wildly** (69.3% ‚Üí 93.1% ‚Üí 54.0% ‚Üí 97.9%)
- Domain model remains **rock-solid** at 99.9% throughout
- Both flip to "Complete" when the **period** arrives

---

### Key Insights: Streaming Behavior

1. **Confidence Stability**
   - General Model: 95-98% typical, drops to 54-69% on complex phrases
   - Domain Model: Consistent 99.9% confidence until final decision

2. **Decision Triggers**
   - Both models wait for punctuation (`.`, `?`) before predicting "Complete"
   - Without punctuation, both maintain "Incomplete"

3. **Implication for Production**
   - Domain model's stability means fewer false signals to downstream systems
   - General model's fluctuations could trigger premature actions in state machines

---

## ‚ö†Ô∏è Part 2: The Punctuation Dependency Problem

### Testing Robustness Without Punctuation

Many ASR systems produce text **without reliable punctuation**, especially in streaming mode. If our models depend on punctuation cues rather than semantic understanding, they'll fail in these scenarios.

**Test Method:**
- Take the same sentence
- Remove **all punctuation** and lowercase
- Compare predictions

---

### Results: Punctuation Sensitivity

| Example | True Label | Model | Original (with punct.) | Normalized (no punct.) | Changed? |
|---------|------------|-------|----------------------|----------------------|----------|
| "Is it a 250 or 350?" | Complete | General | Complete (97.9%) | **Incomplete** (92.0%) | ‚ùå YES |
| | | Domain | Complete (100%) | **Incomplete** (99.9%) | ‚ùå YES |
| "Yep, I can do that." | Complete | General | Complete (84.1%) | **Incomplete** (54.2%) | ‚ùå YES |
| | | Domain | Complete (100%) | **Incomplete** (99.9%) | ‚ùå YES |
| "Yeah that SPX is an option." | Complete | General | Complete (97.3%) | Complete (64.5%) | ‚úì Same |
| | | Domain | Complete (100%) | **Incomplete** (99.9%) | ‚ùå YES |
| "We were hoping kind of" | Incomplete | General | Complete (96.5%) | Complete (96.6%) | ‚úì Same |
| | | Domain | Incomplete (99.9%) | Incomplete (99.9%) | ‚úì Same |

---

### Summary: Punctuation Dependency Detected

**Prediction Changes (when punctuation removed):**
- **General Model:** 2/6 examples changed (33.3%)
- **Domain Model:** 4/6 examples changed (66.7%)

**Average Confidence Impact:**
- **General Model:** 15.9% average drop
- **Domain Model:** 0.0% average drop (but predictions flip entirely)

---

### Critical Finding: Domain Model is MORE Punctuation-Dependent

**Counterintuitive Result:** The Domain model, despite its higher accuracy and confidence, is **more sensitive** to punctuation removal than the General model.

**Why This Matters:**
- Domain model learned **sharper decision boundaries** based on call center patterns
- Those patterns include punctuation as a strong signal
- When punctuation is removed, the model loses its primary cue

**Examples:**
- `"Is it a 250 or 350?"` ‚Üí Complete (perfect)
- `"is it a 250 or 350"` ‚Üí **Incomplete** (wrong)

The question mark isn't just helpful - it's **essential** to the Domain model's decision.

---

## üî¨ Interpretation: What Did Our Models Actually Learn?

### The Uncomfortable Truth: We Built a Punctuation Detector

Our models didn't learn to detect turn completion - they learned to detect punctuation.

**The circular dependency:**
1. ASR system uses acoustic/prosodic cues to decide where turns end
2. ASR inserts punctuation (`.`, `?`, `!`) based on those turn boundaries
3. Our model reads the punctuation and predicts "Complete"
4. We claim we're doing "turn detection" but we're just echoing what the ASR already decided

**We're adding latency and complexity to return the answer the ASR already knew.**

### Evidence: The Models Collapse Without Punctuation

Short, complete utterances **fail entirely** without punctuation:
- `"Yep, I can do that."` ‚Üí Complete ‚úì (correct)
- `"yep i can do that"` ‚Üí **Incomplete** ‚ùå (both models wrong)

The models learned a simple rule:
- **Period or Question Mark = Complete**
- **No punctuation = Incomplete**

This appears to be pattern matching on punctuation characters rather than semantic understanding.

### The One Exception: Trailing Phrases

Some examples remain stable without punctuation:
- `"We were hoping kind of"` ‚Üí Both models correctly predict Incomplete
- The trailing phrase "kind of" signals incompleteness even without punctuation

But this is the **minority case**. Most predictions depend entirely on punctuation.

---

## üí° Implications for Real-World Deployment

### Scenario 1: High-Quality ASR with Punctuation
‚ö†Ô∏è **Our model is redundant**
- ASR already detected the turn boundary (that's why it inserted the period)
- Our model just confirms what the ASR decided
- We're adding latency (~50ms) to echo the ASR's answer
- **No value added**

### Scenario 2: Streaming ASR Without Punctuation
‚ùå **Our models are useless**
- Domain model: 66.7% of predictions change without punctuation
- General model: 33.3% of predictions change
- Both models will predict "Incomplete" until punctuation arrives
- Risk of long, awkward pauses waiting for ASR to add punctuation

### Real-World Example: The Failure Mode

```
User says: "Yeah that's fine" [natural ending, complete turn]
ASR outputs (streaming): "yeah" ‚Üí "yeah that's" ‚Üí "yeah that's fine"
Our models predict:       Incomplete ‚Üí Incomplete ‚Üí Incomplete
[Awkward 2-second silence while waiting for punctuation...]
ASR finally adds period: "yeah that's fine."
Our models predict:       Complete!
[Too late - user already repeated themselves or got frustrated]
```

### For Our Model to Be Useful, It Must:

1. **Work without punctuation** (semantic understanding, not punctuation detection)
2. **Disagree with ASR sometimes** (provide independent signal, catch ASR mistakes)
3. **Use domain knowledge** (understand call center patterns ASR doesn't know)

**Current status:** We fail all three criteria.

---

## üéØ Conclusion: We Built the Wrong Thing (But That's Okay)

Our models achieved **100% test accuracy**, but this chapter reveals what they actually learned:

**We built a punctuation classifier, not a turn detector.**

The models appear to have learned a simple rule rather than semantic understanding of turn completion:
- Ends with `.` or `?` ‚Üí "Complete"
- No punctuation ‚Üí "Incomplete"

### The Real Question for Chapter 3

**Can we force models to learn semantic patterns instead?**

If trained on text with **all punctuation removed**, the models can't rely on these shortcuts. They would need to learn:
- Semantic completeness ("kind of" trails off, "that's fine" is complete)
- Domain-specific ending patterns (call center vocabulary/phrases)
- Syntactic structure (question words, incomplete clauses)

**The test:** Will normalized models achieve reasonable accuracy? Or is punctuation the **only learnable signal** for turn detection from text alone?

**Next Step:** Chapter 3 explores punctuation-agnostic training to test whether semantic learning is possible, or whether text-alone approaches may have fundamental limitations.

---

## üõ†Ô∏è Reproducing These Results

### Run Streaming Analysis
```bash
cd chapter_02_service
python streaming_inference.py
```
Generates `streaming_examples.md` with word-by-word predictions.

### Run Punctuation Robustness Test
```bash
python punctuation_robustness_test.py
```
Generates `punctuation_robustness_results.md` with detailed comparisons.

---

**Key Takeaway:** High test accuracy doesn't guarantee robust deployment. Testing turn detection models **without punctuation** can reveal whether they've learned semantic patterns or relied on punctuation shortcuts.
